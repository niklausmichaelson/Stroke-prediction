# -*- coding: utf-8 -*-
"""Stroke_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_afvH7i8Eu32S3IOkpGNsk_DUqL2Wz65

# Models to implement

1.) Decision Tree Classifier

2.) Random Forest Classifier

3.) Light GBM

4.) Logistic Regression

5.) Gradient boosting classifier

6.) Adaboost Classifier

7.) KNN Classifier

8.) MLP

9.) Xgboost

10.) Boost -> Bagg

11.) Support Vector Classification

# Data and libraries import
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import tree

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/drive/MyDrive/PRML_Dataset/PRML Course Project Files/healthcare-dataset-stroke-data.csv')

df

"""# Preprocessing"""

#droping "id" column
df = df.drop("id",axis=1)
df

#Handeling missing values
print(df.isnull().sum())

#Handeling missing values

df['bmi'] = df['bmi'].fillna(df['bmi'].mean())

#Finding non-numerical columns

col = df.select_dtypes(include=['object']).columns.tolist()
col

#Encoding non-numerical columns
df.gender.unique()
for i in col:
  print("Unique values for", i, "is",df[i].unique())

for i in range(len(df['gender'])):
  if df['gender'][i] == 'Male':
    df['gender'][i] = 0
  elif (df['gender'][i] == 'Female'):
    df['gender'][i] = 1
  else:
    df['gender'][i] = 2

for i in range(len(df['ever_married'])):
  if df['ever_married'][i] == 'Yes':
    df['ever_married'][i] = 0
  else:
    df['ever_married'][i] = 1

for i in range(len(df['work_type'])):
  if df['work_type'][i] == 'Private':
    df['work_type'][i] = 0
  elif (df['work_type'][i] == 'Self-employed'):
    df['work_type'][i] = 1
  elif (df['work_type'][i] == 'Govt_job'):
    df['work_type'][i] = 2
  elif (df['work_type'][i] == 'children'):
    df['work_type'][i] = 3
  elif (df['work_type'][i] == 'Never_worked'):
    df['work_type'][i] = 4

for i in range(len(df['Residence_type'])):
  if df['Residence_type'][i] == 'Urban':
    df['Residence_type'][i] = 0
  else:
    df['Residence_type'][i] = 1

for i in range(len(df['smoking_status'])):
  if df['smoking_status'][i] == 'formerly smoked':
    df['smoking_status'][i] = 0
  elif (df['smoking_status'][i] == 'never smoked'):
    df['smoking_status'][i] = 1
  elif (df['smoking_status'][i] == 'smokes'):
    df['smoking_status'][i] = 2
  else:
    df['smoking_status'][i] = 3

df

#Defining y as target and x as features
x = df.iloc[:,0:10]
y = df.iloc[:,10]

#splitting data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)

# As different features has different scaling or range, we need to do scaling for better accuracy, hence scaled testing as well as training dataset using standard scalar

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train = pd.DataFrame(x_train)
x_test = pd.DataFrame(x_test)

X_train = x_train
Y_train = y_train
X_test = x_test
Y_test = y_test

np.unique(df['stroke'], return_counts=True)

"""# Visualisation"""

sns.pairplot(df, hue='stroke')

print(df["stroke"].value_counts())
df["stroke"].value_counts().plot(kind="pie", autopct='%1.1f%%', figsize=(7,7));

g = sns.PairGrid(df)
g.map_upper(sns.histplot)
g.map_lower(sns.kdeplot, fill=True)
g.map_diag(sns.histplot, kde=True)

sns.set(rc = {'figure.figsize':(25,15)})
sns.heatmap(df.corr(), annot=True)

sns.displot(df, x="gender", hue="stroke", element="step").add_legend()
print("gender = 1 is male and gender = 0 is female")
sns.displot(df, x="age", hue="stroke", element="step")
sns.displot(df, x="hypertension", hue="stroke", element="step")
sns.displot(df, x="heart_disease", hue="stroke", element="step")
sns.displot(df, x="ever_married", hue="stroke", element="step")
sns.displot(df, x="work_type", hue="stroke", element="step")
sns.displot(df, x="Residence_type", hue="stroke", element="step")
sns.displot(df, x="avg_glucose_level", hue="stroke", element="step")
sns.displot(df, x="bmi", hue="stroke", element="step")
sns.displot(df, x="smoking_status", hue="stroke", element="step")
sns.displot(df, x="stroke", hue="stroke", element="step")

sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'gender').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'age').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'hypertension').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'heart_disease').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'ever_married').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'work_type').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'Residence_type').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'avg_glucose_level').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'bmi').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'smoking_status').add_legend()
sns.FacetGrid(df, hue='stroke', size=5).map(sns.distplot,'stroke').add_legend()

# for col in df.columns:
#   sns.boxplot(x=col, y='stroke', data=df)
sns.set(rc={'figure.figsize':(5,5)})
sns.boxplot(x='gender',y='stroke',data=df)

sns.boxplot(x='age',y='stroke',data=df)

sns.boxplot(x='hypertension',y='stroke',data=df)

sns.boxplot(x='heart_disease',y='stroke',data=df)

sns.boxplot(x='ever_married',y='stroke',data=df)

sns.boxplot(x='work_type',y='stroke',data=df)

sns.boxplot(x='Residence_type',y='stroke',data=df)

sns.boxplot(x='avg_glucose_level',y='stroke',data=df)

sns.boxplot(x='bmi',y='stroke',data=df)

sns.boxplot(x='smoking_status',y='stroke',data=df)

sns.set(rc={'figure.figsize':(5,5)})
sns.boxplot(x='stroke',y='stroke',data=df)

"""# Pipeline"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
import lightgbm as gbm
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix
from sklearn.metrics import roc_curve, auc

import plotly.express as px
svc=SVC(probability=True, kernel='linear')
xgbr =  XGBClassifier(n_estimators= 300, max_depth= 5, learning_rate= 0.15)

lr_pipeline = Pipeline([('Logistic Regression', LogisticRegression())])
knn_pipeline = Pipeline([('K Neighbors Classifier', KNeighborsClassifier(n_neighbors = 14))])
abc_pipeline = Pipeline([('Ada Boost Classifier', AdaBoostClassifier(n_estimators=50, base_estimator=svc, learning_rate=1))])
gbc_pipeline = Pipeline([('Gradient Boosting Classifier', GradientBoostingClassifier())])
xgb_pipeline = Pipeline([('XGBoost Classifier', XGBClassifier())])
booba_pipeline = Pipeline([('Boost->Bagging Method',  BaggingClassifier(base_estimator=xgbr, random_state=0))])
svc_pipeline = Pipeline([('Support Vector Classification', SVC(probability= True))])
svc_pipeline1 = Pipeline([('Support Vector Classification', SVC(probability= True, C=1, decision_function_shape= 'ovo', degree= 2, gamma= 'scale'))])
dtc_pipeline = Pipeline([('Decision Tree Classifier', DecisionTreeClassifier())])
rfc_pipeline = Pipeline([('Random Forest Classifier', RandomForestClassifier(max_depth=2, random_state=0))])
lgbm_pipeline = Pipeline([('Light gbm', gbm.LGBMClassifier(learning_rate=0.25,n_estimators=800,max_depth=10,cv=5))])

def access_pipeline(pipeline, x_train, y_train, x_test,y_test):
    pipeline.fit(x_train, y_train)
    y_pred = pipeline.predict(x_test)
    print("\nAccuracy from the model for testing data is", accuracy_score(y_test, y_pred)*100, "%.\n")
    print("Confusion Matrix for the model is :\n")
    plot_confusion_matrix(pipeline, x_test, y_test)
    plt.show()
    print("\nClassification report :\n")
    print(classification_report(y_test, y_pred))
    print("\nROC and AUC :\n")
    y_predicted = pipeline.predict_proba(x_test)[:,1]
    fpr, tpr, thresholds = roc_curve(y_test, y_predicted)
    roc_fig = px.area(x=fpr, y=tpr,title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',labels=dict(x='False Positive Rate', y='True Positive Rate'),width=800, height=500,template='plotly_dark')
    roc_fig.add_shape(type='line', line=dict(dash='dash'),x0=0, x1=1, y0=0, y1=1)
    roc_fig.update_yaxes(scaleanchor="x", scaleratio=1)
    roc_fig.update_xaxes(constrain='domain')
    
    roc_fig.show()

"""#Decision Tree Classifier"""

access_pipeline(dtc_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
    'criterion': ['gini', 'entropy'],
    'max_depth' : [2, 4, 6, 8, 12], 
    'min_samples_leaf' : [5, 10, 20, 50, 100]
}

clf_dtr = GridSearchCV(estimator= DecisionTreeClassifier(),                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='accuracy',        # metric for scoring
                   cv=10)

clf_dtr.fit(x_train,y_train)

print("Tuned Hyperparameters :", clf_dtr.best_params_)

dtc_pipeline1 = Pipeline([('Decision Tree Classifier', DecisionTreeClassifier(criterion = 'gini', max_depth = 2, min_samples_leaf = 5))])

access_pipeline(dtc_pipeline1, x_train, y_train, x_test,y_test)

"""#Random Forest Classifier"""

access_pipeline(rfc_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
    'bootstrap': [True],
    'max_depth': [70, 80, 100],
    'min_samples_leaf': [3, 4, 5],
    'n_estimators': [100, 200, 300, 1000]
}

clf_rfc = GridSearchCV(estimator= RandomForestClassifier(),                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='accuracy',        # metric for scoring
                   cv=10)

clf_rfc.fit(x_train, y_train)

print("Tuned Hyperparameters :", clf_rfc.best_params_)

rfc_pipeline1 = Pipeline([('Random Forest Classifier', RandomForestClassifier( bootstrap = True,  max_depth = 80,  max_features = 2,  min_samples_leaf = 3,  min_samples_split = 8,  n_estimators = 200))])

access_pipeline(rfc_pipeline1, x_train, y_train, x_test,y_test)

"""# Lightgbm"""

access_pipeline(lgbm_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}

clf_lgbm = GridSearchCV(estimator= gbm.LGBMClassifier(),                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='accuracy',        # metric for scoring
                   cv=10)

clf_lgbm.fit(x_train, y_train)

print("Tuned Hyperparameters :", clf_lgbm.best_params_)

lgbm_pipeline1 = Pipeline([('Light gbm', gbm.LGBMClassifier( bootstrap = True,  max_depth = 80,  max_features = 2,  min_samples_leaf = 3,  min_samples_split = 8,  n_estimators = 100))])

access_pipeline(lgbm_pipeline1, x_train, y_train, x_test,y_test)

"""# Logistic Regression"""

access_pipeline(lr_pipeline, x_train, y_train, x_test, y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
    'penalty' : ['l1','l2'], 
    'C'       : np.linspace(-3,3,100),
    'solver'  : ['newton-cg', 'lbfgs', 'liblinear']
}

clf = GridSearchCV(lr_pipeline,                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='accuracy',        # metric for scoring
                   cv=10)                     # number of folds

clf.fit(x_train,y_train)

print("Tuned Hyperparameters :", clf.best_params_)

lr_pipeline1 = Pipeline([('Logistic Regression', LogisticRegression(C= 0.030303030303030276, penalty= 'l1', solver= 'liblinear'))])

access_pipeline(lr_pipeline1, x_train, y_train, x_test, y_test)

"""# K Nearest Neighbours"""

access_pipeline(knn_pipeline, x_train, y_train, x_test, y_test)

neighbours = range(1,30)
test_acc = []
train_acc = []
op_ngh = 0
max_acc = 0
for i in neighbours:
  knn_pipeline = KNeighborsClassifier(n_neighbors = i)
  knn_pipeline.fit(x_train, y_train)
  acc_trn = knn_pipeline.score(x_train, y_train)
  acc_tst = knn_pipeline.score(x_test, y_test)
  train_acc.append(acc_trn)
  test_acc.append(acc_tst)
  if(acc_tst > max_acc):
    max_acc = acc_tst
    op_ngh = i

print("Optimal n_neighbors for knn is", op_ngh,"and it gives", max_acc,"of testing accuracy")

fig = plt.subplots(figsize=(8, 5))
plt.plot(range(1, 30), train_acc, label = 'Training accuracy')
plt.plot(range(1, 30), test_acc, label = 'Testing accuracy')
plt.legend()
plt.xlabel('n_neighbors')
plt.ylabel('Accuracy')
plt.show()

leaf_sizes = range(1,30)
test_acc1 = []
train_acc1 = []
op_leaf_size = 0
max_acc1 = 0
for i in leaf_sizes:
  knn_pipeline = KNeighborsClassifier(n_neighbors = op_ngh, leaf_size = i)
  knn_pipeline.fit(x_train, y_train)
  acc_trn = knn_pipeline.score(x_train, y_train)
  acc_tst = knn_pipeline.score(x_test, y_test)
  train_acc1.append(acc_trn)
  test_acc1.append(acc_tst)
  if(acc_tst > max_acc1):
    max_acc1 = acc_tst
    op_leaf_size = i

print("Optimal leaf_sizes for knn is", op_leaf_size,"and it gives", max_acc1,"of testing accuracy.")

fig = plt.subplots(figsize=(8, 5))
plt.plot(range(1, 30), train_acc1, label = 'Training accuracy')
plt.plot(range(1, 30), test_acc1, label = 'Testing accuracy')
plt.legend()
plt.xlabel('leaf_size')
plt.ylabel('Accuracy')
plt.show()

knn_pipeline2 = Pipeline([('K Neighbors Classifier', KNeighborsClassifier(n_neighbors = op_ngh))])

access_pipeline(knn_pipeline2, x_train, y_train, x_test, y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
 'leaf_size' :  list(range(1,50)),
 'n_neighbors' :  list(range(1,30)),
 'p': [1,2]
}

clf = GridSearchCV(knn_pipeline,param_grid = parameters,cv=10)

clf.fit(x_train,y_train)

print("Tuned Hyperparameters :", clf.best_params_)

knn_pipeline3 = Pipeline([('K Neighbors Classifier', KNeighborsClassifier(leaf_size = 1, n_neighbors = 12, p = 2))])

access_pipeline(knn_pipeline3, x_train, y_train, x_test, y_test)

"""# Gradient Boosting Classifier"""

access_pipeline(gbc_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearchCV"""

from sklearn.model_selection import GridSearchCV


parameters = {
 'n_estimators' : range(2,25), 'learning_rate' : [0.3, 0.5, 0.8, 1.5], 'max_features' : [2, 3, 5, 4], 'max_depth' : [2, 5, 8, 13]
}


clf = GridSearchCV(gbc_pipeline,param_grid = parameters,cv=10)

clf.fit(x_train,y_train)

print("Tuned Hyperparameters :", clf.best_params_)

gbc_pipeline1 = Pipeline([('Gradient Boosting Classifier', GradientBoostingClassifier(learning_rate = 0.5, max_depth = 2, max_features = 2, n_estimators = 4))])

access_pipeline(gbc_pipeline1, x_train, y_train, x_test,y_test)

"""# AdaBoost Classifier"""

access_pipeline(abc_pipeline, x_train, y_train, x_test,y_test)

"""#### GirdSearchCV"""

from sklearn.model_selection import GridSearchCV

parameters = {
 'n_estimators' : range(2,30), 'learning_rate' : np.linspace(0.1,2,19)
}

abc_pipeline1 = Pipeline([('Ada Boost Classifier', AdaBoostClassifier())])
clf = GridSearchCV(abc_pipeline1,param_grid = parameters,cv=10)

clf.fit(x_train,y_train)

print("Tuned Hyperparameters :", clf.best_params_)
print("Accuracy :",clf.best_score_)

abc_pipeline2 = Pipeline([('Ada Boost Classifier', AdaBoostClassifier(n_estimators=23, base_estimator=svc, learning_rate=1.3666666666666667))])

access_pipeline(abc_pipeline2, x_train, y_train, x_test,y_test)

"""# MLP"""

import torch #import python #
import keras #
import tensorflow #import pytorch
import torch.nn as nn
from torch.autograd import Variable

import pandas as pd

from sklearn.utils import shuffle



X_train=X_train.values
Y_train=Y_train.values
X_test=X_test.values
Y_test=Y_test.values

def get_accuracy(logit, target, batch_size):
    ''' Obtain accuracy for training round '''
    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()
    accuracy = 100.0 * corrects/batch_size
    return accuracy.item()

#Define training hyperprameters.
batch_size = 32 #sample batch
num_epochs = 1000 #number times dataset seen
learning_rate = 0.01
size_hidden = 1024 #neurons
# size_hidden_2 = 100 #neurons
num_classes = 30

#Calculate some other hyperparameters based on data.  
batch_no = len(X_train) // batch_size  #batches
cols = X_train.shape[1] #Number of columns in input matrix

#Create the model
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class Net(torch.nn.Module):
    def __init__(self, num_inputs, size_hidden, n_output):
        super(Net, self).__init__()
        self.hidden_1 = nn.Linear(num_inputs, size_hidden)
        self.hidden_2 = nn.Linear(num_inputs, size_hidden)
        self.hidden_actvn = nn.Tanh()

        self.out_layer = nn.Linear(size_hidden, n_output)
        self.out_actvn = nn.Identity()

    def forward(self, x):
        x1 = self.hidden_actvn(self.hidden_1(x))      # activation function for hidden layer
        x2= self.hidden_actvn(self.hidden_2(x))      # activation function for hidden layer
        x3 = torch.add(x2,x1)
        res = self.out_actvn(self.out_layer(x3))                    # output
        return res
        

net = Net(cols, size_hidden, num_classes)
# summary(net, (1, 4))

optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)
loss_func = torch.nn.CrossEntropyLoss()

# Commented out IPython magic to ensure Python compatibility.
for epoch in range(num_epochs):
    #Shuffle just mixes up the dataset between epocs
    X_train, Y_train = shuffle(X_train, Y_train)

    train_acc = 0.0
    running_loss = 0.0

    # Mini batch learning
    for i in range(batch_no):
        start = i * batch_size
        end = start + batch_size
        inputs = torch.FloatTensor(X_train[start:end])
        labels = torch.LongTensor(Y_train[start:end])
        
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)

        # print(labels)
        # loss = criterion(outputs, torch.unsqueeze(labels, dim=1))
        # print(max(labels))
        loss = loss_func(outputs, labels)
        
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        acc = get_accuracy(outputs, labels, batch_size)
        train_acc += acc
         
    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \
#           %(epoch+1, running_loss / (i+1), train_acc/(i+1)))  
    running_loss = 0.0

"""# Xgboost"""

access_pipeline(xgb_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearch"""

from sklearn.model_selection import GridSearchCV

params = { 'max_depth': [5, 7, 9],
           'n_estimators': [300, 500, 900],
           'learn_rate': [0.15, 0.45, 0.75]}
xgbc = XGBClassifier()


clf = GridSearchCV(estimator=xgbc, 
                   param_grid=params,
                   scoring='neg_mean_squared_error', 
                   verbose=1,   
                   cv=5)
clf.fit(X_train, y_train)
# clf_train_pred = clf.predict(x_test)
#print("accuracy score train", accuracy_score(Y_test, clf_pred))
print("Best parameters:", clf.best_params_)
print("RMSE train: ", (-clf.best_score_)**(1/2.0))

xgb_pipeline1 = Pipeline([('XGBoost Classifier', XGBClassifier(n_estimators= 300, max_depth= 5, learning_rate= 0.15))]) # best params

access_pipeline(xgb_pipeline1, x_train, y_train, x_test,y_test)

"""# Boosting -> Bagging"""

xgbr =  XGBClassifier(n_estimators= 300, max_depth= 5, learning_rate= 0.15)

access_pipeline(booba_pipeline, x_train, y_train, x_test,y_test)

"""# Support Vector Classification"""

access_pipeline(svc_pipeline, x_train, y_train, x_test,y_test)

"""#### GridSearch CV"""

params={
    'C': [1, 3, 5, 7, 9],
    # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],
    'degree': [2, 3, 4, 5, 6],
    'gamma': ['scale', 'auto'],
    'decision_function_shape': ['ovo', 'ovr']
}

sv = SVC()


clf = GridSearchCV(estimator= sv, 
                   param_grid=params,
                   scoring='neg_mean_squared_error', 
                   verbose=1,   
                   cv=5)
clf.fit(X_train, y_train)
# clf_train_pred = clf.predict(x_test)
#print("accuracy score train", accuracy_score(Y_test, clf_pred))
print("Best parameters:", clf.best_params_)
print("RMSE train: ", (-clf.best_score_)**(1/2.0))

access_pipeline(svc_pipeline1, x_train, y_train, x_test,y_test)